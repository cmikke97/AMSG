[sorel20mDataset]
# max number of training data samples to use (if -1 -> takes all), default: 6000000
training_n_samples = 6000000
# max number of validation data samples to use (if -1 -> takes all), default: 1153846
validation_n_samples = 1153846
# max number of test data samples to use (if -1 -> takes all), default: 1846154
test_n_samples = 1846154

# This is the timestamp that divides the validation data (used to check convergence/overfitting)
# from test data (used to assess final performance)
validation_test_split = 1547279640.0
# This is the timestamp that splits training data from validation data
train_validation_split = 1543542570.0
# total number of available samples in the original Sorel20M dataset
total_training_samples = 12699013
total_validation_samples = 2495822
total_test_samples = 4195042

[detectionBase]
# set this to the desired device, e.g. 'cuda:0' if a GPU is available, otherwise 'cpu'
device = cuda:0

# set number of workers to be used (if 0 -> set to current system cpu count)
workers = 8

# number of training runs to do
runs = 3
# how many samples per batch to load
batch_size = 8192
# how many epochs to train for
epochs = 10

# whether or not (1/0) to use malware/benignware labels as a target
use_malicious_labels = 1
# whether or not (1/0) to use the counts as an additional target
use_count_labels = 1
# whether or not (1/0) to use the tags as additional targets
use_tag_labels = 1

# define detectionBase net initial linear layers sizes (and amount). Examples:
# - [512,512,128]: the initial layers (before the task branches) will be 3 with sizes 512, 512, 128 respectively
# - [512,256]: the initial layers (before the task branches) will be 2 with sizes 512, 256 respectively
layer_sizes = [512,512,128]

# dropout probability between the first detectionBase net layers
dropout_p = 0.05

# activation function between the first detectionBase net layers. Possible values:
# - elu: Exponential Linear Unit activation function
# - leakyRelu: leaky Relu activation function
# - pRelu: parametric Relu activation function (better to use this with weight decay = 0)
# - relu: Rectified Linear Unit activation function
activation_function = elu

# label weights to be used during loss calculation
# (Notice: only the weights corresponding to enabled labels will be used)
loss_weights = {'malware': 1.0, 'count': 0.1, 'tags': 1.0}

# optimizer to use during training. Possible values:
# - adam: Adam algorithm
# - sgd: stochastic gradient descent
optimizer = adam

# learning rate to use during training
lr = 0.001

# momentum to be used during training when using 'sgd' optimizer
momentum = 0.0

# weight decay (L2 penalty) to use with selected optimizer
weight_decay = 0.0

# generator type. Possible values are:
# - base: use basic generator (from the original SOREL20M code) modified to work with the pre-processed dataset
# - alt1: use alternative generator 1. Inspired by the 'index select' version of
#         https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6, this version uses a new
#         dataloader class, called FastTensorDataloader, to process tabular based data. It was modified from the
#         original version available at the above link to be able to work with the pre-processed dataset (numpy memmap)
#         and with multiple workers (in multiprocessing).
# - alt2: use alternative generator 2. Inspired by the 'shuffle in-place' version of
#         https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6, this version uses a new
#         dataloader class, called FastTensorDataloader, to process tabular based data. It was modified from the
#         original version available at the above link to be able to work with the pre-processed dataset (numpy memmap)
#         and with multiple workers (in multiprocessing).
# - alt3: use alternative generator 3. This version uses a new dataloader class, called FastTensorDataloader which
#         asynchronously (if workers > 1) loads the dataset into memory in randomly chosen chunks which are
#         concatenated together to form a 'chunk aggregate' -> the data inside a chunk aggregate is then shuffled.
#         Finally batches of data are extracted from a chunk aggregate. The samples shuffling is therefore more
#         localised but the loading speed is greatly increased.
gen_type = alt3

[jointEmbedding]
# set this to the desired device, e.g. 'cuda:0' if a GPU is available, otherwise 'cpu'
device = cuda:0

# set number of workers to be used (if 0 -> set to current system cpu count)
workers = 8

# number of training runs to do
runs = 3
# how many samples per batch to load # 8192
batch_size = 8192
# how many epochs to train for
epochs = 10

# whether or not (1/0) to use malware/benignware labels as a target
use_malicious_labels = 1
# whether or not (1/0) to use the counts as an additional target
use_count_labels = 1

# define JointEmbedding net initial linear layers sizes (and amount). Examples:
# - [512,512,128]: the initial layers (before the task branches) will be 3 with sizes 512, 512, 128 respectively
# - [512,256]: the initial layers (before the task branches) will be 2 with sizes 512, 256 respectively
layer_sizes = [512,512,128]

# dropout probability between the first JointEmbedding net layers
dropout_p = 0.05

# activation function between the first detectionBase net layers. Possible values:
# - elu: Exponential Linear Unit activation function
# - leakyRelu: leaky Relu activation function
# - pRelu: parametric Relu activation function (better to use this with weight decay = 0)
# - relu: Rectified Linear Unit activation function
activation_function = elu

# label weights to be used during loss calculation
# (Notice: only the weights corresponding to enabled labels will be used)
loss_weights = {'malware': 1.0, 'count': 0.1, 'tags': 1.0}

# optimizer to use during training. Possible values:
# - adam: Adam algorithm
# - sgd: stochastic gradient descent
optimizer = adam

# learning rate to use during training
lr = 0.001

# momentum to be used during training when using 'sgd' optimizer
momentum = 0.0

# weight decay (L2 penalty) to use with selected optimizer
weight_decay = 0.0

# generator type. Possible values are:
# - base: use basic generator (from the original SOREL20M code) modified to work with the pre-processed dataset
# - alt1: use alternative generator 1. Inspired by the 'index select' version of
#         https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6, this version uses a new
#         dataloader class, called FastTensorDataloader, to process tabular based data. It was modified from the
#         original version available at the above link to be able to work with the pre-processed dataset (numpy memmap)
#         and with multiple workers (in multiprocessing)
# - alt2: use alternative generator 2. Inspired by the 'shuffle in-place' version of
#         https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6, this version uses a new
#         dataloader class, called FastTensorDataloader, to process tabular based data. It was modified from the
#         original version available at the above link to be able to work with the pre-processed dataset (numpy memmap)
#         and with multiple workers (in multiprocessing)
# - alt3: use alternative generator 3. This version uses a new dataloader class, called FastTensorDataloader which
#         asynchronously (if workers > 1) loads the dataset into memory in randomly chosen chunks which are
#         concatenated together to form a 'chunk aggregate' -> the data inside a chunk aggregate is then shuffled.
#         Finally batches of data are extracted from a chunk aggregate. The samples shuffling is therefore more
#         localised but the loading speed is greatly increased
gen_type = alt3

# similarity measure used to evaluate distances in joint embedding space. Possible values are:
# - dot: dot product between vectors in the embedding space. The similarity measure used in JointEmbedding paper
# - cosine: cosine similarity between vectors in the embedding space
# - pairwise_distance: calculates the pairwise distance and then transforms it to a similarity measure (between 0 and 1)
similarity_measure = dot

# + IF 'pairwise_distance' IS SELECTED AS similarity_measure -----------------------------------------------------------
# |
# | distance-to-similarity function to use. These functions will map values belonging to the R+ set (Real positives) to
# | real values belonging to the [0,1] interval. Possible values are:
# | - exp: will compute e^(-x/a)
# | - inv: will compute 1/(1+x/a)
# | - inv_pow: will compute 1/(1+(x^2)/a)
# | where 'a' is a multiplicative factor (see 'pairwise_a')
pairwise_distance_to_similarity_function = exp
# |
# | distance-to-similarity function 'a' multiplicative factor
pairwise_a = 1.0
# + --------------------------------------------------------------------------------------------------------------------

[freshDataset]
# Specify Malware Bazaar families of interest.
# NOTE: It is recommended to specify more families than 'number_of_families' since Malware Bazaar may not have
# 'amount_each' samples for some of them. These families will be considered in order.

# Taken from https://cert-agid.gov.it/tag/riepilogo/
signatures = Heodo, AgentTesla, Formbook, MassLogger, Loki, AveMariaRAT, NjRAT, RemcosRAT, NetWire, AZORult, Smoke Loader, Matiex, AsyncRAT, CobaltStrike, GuLoader, NanoCore, OskiStealer, SnakeKeylogger, RaccoonStealer, TrickBot, BitRAT
# Number of families to consider. The ones in excess, going in order, will not be considered.
number_of_families = 10
# Amount of samples for each malware family to retrieve from Malware Bazaar
amount_each = 100

# Number of queries to do in model ranking evaluation
queries = 100
# Number of anchor samples per family to use in model prediction evaluation
n_anchor_samples_per_family = 10
# Whether to perform a 'hard' (1) or 'soft' (0) malware family prediction
hard = 1

[model_refinement]
runs = 3
epochs = 20

train_split_proportion = 8
valid_split_proportion = 1
test_split_proportion = 1

batch_size = 1000

# learning rate to use during refinement
lr = 0.001

# momentum to be used during training when using 'sgd' optimizer
momentum = 0.0

# weight decay (L2 penalty) to use with selected optimizer
weight_decay = 0.0